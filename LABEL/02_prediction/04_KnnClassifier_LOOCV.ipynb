{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Limit the number of threads used by various linear algebra libraries\n",
    "# to avoid oversubscription and improve efficiency in multiprocessing environments\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 128\n",
    "x_train = np.load(f'/home/lbh/projects_dir/BigSlice/dataset/uni_feature_{patch_size}_all.npy')\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full annotation table with 'spot' as index\n",
    "all_df = pd.read_csv('/home/lbh/projects_dir/BigSlice/Celltype_Annotations/all_annotations.csv', index_col=0)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_skin_subregion(df):\n",
    "    \"\"\"\n",
    "    Update subregion annotations for 'SK' (skin) organ entries.\n",
    "    Replaces missing or 'other'-like subregion values with 'Skin_other'.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with 'organ' and 'subregion' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame.\n",
    "        int: Number of rows modified.\n",
    "    \"\"\"\n",
    "    # Condition: rows where organ is 'SK' and subregion is NA or 'other'\n",
    "    condition = (df['organ'] == 'SK') & (\n",
    "        df['subregion'].isna() | (df['subregion'].astype(str).str.lower() == 'other')\n",
    "    )\n",
    "    num_updated = condition.sum()\n",
    "\n",
    "    # Apply update: set subregion to 'Skin_other' for matching rows\n",
    "    df.loc[condition, 'subregion'] = 'Skin_other'\n",
    "    return df, num_updated\n",
    "\n",
    "# Apply the update to training annotations\n",
    "subset_df_sorted, num_updated_train = update_skin_subregion(all_df)\n",
    "\n",
    "# Print the number of updated rows\n",
    "print(f\"Train set updated: {num_updated_train} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(all_df['subregion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def leave_one_batch_out(features: np.ndarray, df_labels: pd.DataFrame, key='batch'):\n",
    "    \"\"\"\n",
    "    Generator for leave-one-batch-out cross validation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    features : np.ndarray\n",
    "        N x D feature matrix (N samples, D features).\n",
    "    df_labels : pd.DataFrame\n",
    "        DataFrame with columns including \"batch\".\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    batch_id : the held-out batch identifier\n",
    "    train_df : DataFrame for training\n",
    "    val_df   : DataFrame for validation\n",
    "    train_features : np.ndarray for training\n",
    "    val_features   : np.ndarray for validation\n",
    "    \"\"\"\n",
    "    assert len(features) == len(df_labels), \"Features and df_labels must align row-wise.\"\n",
    "    \n",
    "    batch_list = df_labels[key].unique()\n",
    "    \n",
    "    for batch_id in batch_list:\n",
    "        train_df = df_labels.loc[df_labels[key] != batch_id].reset_index(drop=True)\n",
    "        val_df   = df_labels.loc[df_labels[key] == batch_id].reset_index(drop=True)\n",
    "        \n",
    "        train_mask = df_labels[key] != batch_id\n",
    "        val_mask   = df_labels[key] == batch_id\n",
    "        \n",
    "        train_features = features[train_mask.values]\n",
    "        val_features   = features[val_mask.values]\n",
    "        \n",
    "        yield batch_id, train_df, val_df, train_features, val_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# ==============================\n",
    "# Config\n",
    "# ==============================\n",
    "BASE_K = 5  # global K (will be clipped to [1, n_train])\n",
    "LABEL_COLUMNS: List[str] = [\"organ\", \"subregion\", \"level1_annotation\", \"level2_annotation\", \"level0_annotation\"]\n",
    "\n",
    "USE_COORDINATES = True        # whether to append (x_scaled_image, y_scaled_image) to features\n",
    "N_COMPONENTS_PCA = 200        # PCA dims\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Paths\n",
    "EVAL_DIR = \"/home/lbh/projects_dir/BigSlice/evalset\"\n",
    "\n",
    "# ==============================\n",
    "# Utils\n",
    "# ==============================\n",
    "def safe_name(s: str) -> str:\n",
    "    \"\"\"Make a name safe for filesystem paths.\"\"\"\n",
    "    return \"\".join([c if c.isalnum() or c in \"-_.\" else \"_\" for c in str(s)])\n",
    "\n",
    "def build_features_with_optional_coords(df: pd.DataFrame, feat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Optionally append (x_scaled_image, y_scaled_image) to the features.\"\"\"\n",
    "    if USE_COORDINATES:\n",
    "        if not {\"x_scaled_image\", \"y_scaled_image\"}.issubset(df.columns):\n",
    "            raise KeyError(\"Missing 'x_scaled_image' and/or 'y_scaled_image' in dataframe.\")\n",
    "        coo = df[[\"x_scaled_image\", \"y_scaled_image\"]].values\n",
    "        return np.hstack((coo, feat))\n",
    "    return feat\n",
    "\n",
    "def fit_preprocessor(train_feat: np.ndarray):\n",
    "    \"\"\"Fit StandardScaler and PCA on training features and return transformed train features.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    train_norm = scaler.fit_transform(train_feat)\n",
    "\n",
    "    pca = PCA(n_components=N_COMPONENTS_PCA, random_state=RANDOM_STATE)\n",
    "    X_tr = pca.fit_transform(train_norm)\n",
    "    return scaler, pca, X_tr\n",
    "\n",
    "def transform_features(scaler: StandardScaler, pca: PCA, feat: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply fitted scaler and PCA to features.\"\"\"\n",
    "    return pca.transform(scaler.transform(feat))\n",
    "\n",
    "def uniform_vote(neigh_labels: np.ndarray, n_classes: int) -> (np.ndarray, np.ndarray):\n",
    "    \"\"\"\n",
    "    Uniform voting (equivalent to KNN with weights='uniform').\n",
    "    neigh_labels: (n_val, k) encoded neighbor labels\n",
    "    Returns:\n",
    "        pred_enc: (n_val,) predicted class indices\n",
    "        conf: (n_val,) top1 frequency in [0,1]\n",
    "    \"\"\"\n",
    "    votes = np.eye(n_classes, dtype=np.int64)[neigh_labels].sum(axis=1)  # (n_val, C)\n",
    "    pred_enc = votes.argmax(axis=1)\n",
    "    conf = votes.max(axis=1) / neigh_labels.shape[1]\n",
    "    return pred_enc, conf\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Main loop\n",
    "# Assumes you already have: leave_one_batch_out(x_train, all_df, key='batch')\n",
    "# which yields: (batch_id, train_df, val_df, train_feat, val_feat)\n",
    "# ==============================\n",
    "for batch_id, train_df, val_df, train_feat, val_feat in leave_one_batch_out(\n",
    "    x_train, all_df, key=\"batch\"\n",
    "):\n",
    "    print(f\"Batch {batch_id} | train {train_feat.shape} | val {val_feat.shape}\")\n",
    "\n",
    "    # 1) Build features (optionally append coordinates), then fit scaler & PCA\n",
    "    train_feat_coo = build_features_with_optional_coords(train_df, train_feat)\n",
    "    val_feat_coo   = build_features_with_optional_coords(val_df,   val_feat)\n",
    "\n",
    "    scaler, pca, X_tr = fit_preprocessor(train_feat_coo)\n",
    "    X_va = transform_features(scaler, pca, val_feat_coo)\n",
    "\n",
    "    print(f\"Data preprocessed. Train shape: {X_tr.shape}, Val shape: {X_va.shape}\")\n",
    "\n",
    "    # Prepare output directory & persist preprocessors\n",
    "    output_dir = f\"hierarchical_knn_batch_{safe_name(batch_id)}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    joblib.dump(pca,    os.path.join(output_dir, \"pca_model.pkl\"))\n",
    "    joblib.dump(scaler, os.path.join(output_dir, \"scaler.pkl\"))\n",
    "\n",
    "    # 2) Precompute neighbors ONCE and reuse for every level\n",
    "    k = max(1, min(BASE_K, len(X_tr)))  # clip k to [1, n_train]\n",
    "    nn = NearestNeighbors(n_neighbors=k, n_jobs=-1)\n",
    "    t0 = time.time()\n",
    "    nn.fit(X_tr)\n",
    "    distances, indices = nn.kneighbors(X_va, return_distance=True)\n",
    "    print(f\"Neighbor graph built in {time.time() - t0:.2f}s | k={k}\")\n",
    "\n",
    "    # Optionally persist neighbor graph for reproducibility / analysis\n",
    "    np.save(os.path.join(output_dir, \"val_knn_indices.npy\"), indices)\n",
    "    np.save(os.path.join(output_dir, \"val_knn_distances.npy\"), distances)\n",
    "\n",
    "    # 3) Prepare result frame (keep useful metadata)\n",
    "    predictions_df = val_df.copy()\n",
    "    if \"_row_id\" not in predictions_df.columns:\n",
    "        predictions_df[\"_row_id\"] = predictions_df.index\n",
    "    for c in [\"x_scaled_image\", \"y_scaled_image\"]:\n",
    "        if c not in predictions_df.columns and c in val_df.columns:\n",
    "            predictions_df[c] = val_df[c].values\n",
    "\n",
    "    # 4) Vote per level using the SAME neighbors\n",
    "    print(\"\\n--- Step 2: Vote per level using shared neighbors ---\")\n",
    "    for level in tqdm(LABEL_COLUMNS, desc=\"Levels\"):\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Prepare training labels and encoder\n",
    "        y_tr_str = train_df[level].astype(str).fillna(\"__UNK__\").values\n",
    "        le = LabelEncoder().fit(y_tr_str)\n",
    "        y_tr_enc = le.transform(y_tr_str)            # shape: (n_train,)\n",
    "\n",
    "        # Gather neighbor labels for each validation sample\n",
    "        neigh_labels = y_tr_enc[indices]             # shape: (n_val, k)\n",
    "        n_classes = len(le.classes_)\n",
    "\n",
    "        # Voting\n",
    "        pred_enc, conf = uniform_vote(neigh_labels, n_classes)\n",
    "\n",
    "        y_pred = le.inverse_transform(pred_enc)\n",
    "\n",
    "        # Save predictions and a simple confidence score\n",
    "        predictions_df[f\"{level}_pred\"] = y_pred\n",
    "        predictions_df[f\"{level}_pred_conf\"] = conf\n",
    "\n",
    "        # Persist class list for this level to decode later if needed\n",
    "        joblib.dump(le.classes_, os.path.join(output_dir, f\"classes_{safe_name(level)}.pkl\"))\n",
    "        print(f\"Level [{level}] done in {time.time() - t1:.2f}s | classes={n_classes}\")\n",
    "\n",
    "    # 5) Save predictions\n",
    "    os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "    out_csv = os.path.join(EVAL_DIR, f\"uni_prediction_{safe_name(batch_id)}_LOOCV.csv\")\n",
    "    predictions_df.to_csv(out_csv, index=False)\n",
    "    print(f\"âœ… Done. Saved predictions to: {out_csv}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
